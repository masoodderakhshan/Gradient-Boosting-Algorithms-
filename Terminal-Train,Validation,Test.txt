=== Loading input features ===
   Test  NS  Depth      W  RWD  Density
0     1  20    2.5  29.30  3.5    1.485
1     2  20    7.5  34.63  2.5    1.235
2     3  20   12.5  34.42  2.5    1.310
3     4  20   17.5  34.99  2.5    1.485
4     5  20    7.5  38.14  6.0    1.490

=== Loading displacement data ===

=== Reshaping displacement data ===
   Displacement_Step  Displacement_kPa  Test
0                0.0               0.0     8
1                0.5               5.2     8
2                1.0               6.8     8
3                1.5               8.0     8
4                2.0               8.7     8
   Displacement_Step  Displacement_kPa  Test  NS  Depth      W  RWD  Density
0                0.0               0.0     8  20    2.5  28.88  3.2    1.515
1                0.5               5.2     8  20    2.5  28.88  3.2    1.515
2                1.0               6.8     8  20    2.5  28.88  3.2    1.515
3                1.5               8.0     8  20    2.5  28.88  3.2    1.515
4                2.0               8.7     8  20    2.5  28.88  3.2    1.515

=== Checking missing ===
Displacement_Step    0
Displacement_kPa     0
Test                 0
NS                   0
Depth                0
W                    0
RWD                  0
Density              0
dtype: int64

Train: (1519, 5), Validation: (325, 5), Test: (326, 5)

=== Tuning and evaluating GradientBoostingRegressor ===
Best parameters for GradientBoostingRegressor: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}

=== GradientBoostingRegressor Train Metrics ===
MAE: 0.1509
RMSE: 0.2053
R2: 0.9998

=== GradientBoostingRegressor Validation Metrics ===
MAE: 0.4964
RMSE: 0.8096
R2: 0.9959

=== GradientBoostingRegressor Test Metrics ===
MAE: 0.4303
RMSE: 0.6387
R2: 0.9976

=== GradientBoostingRegressor Cross-validation ===
Cross-validated R² scores: [0.94400852 0.92739762 0.88292903 0.93939662 0.79133755]
Mean R²: 0.8970 | Std: 0.0571

=== Tuning and evaluating XGBoost ===
Best parameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}

=== XGBoost Train Metrics ===
MAE: 0.1710
RMSE: 0.2371
R2: 0.9997

=== XGBoost Validation Metrics ===
MAE: 0.5290
RMSE: 0.8442
R2: 0.9955

=== XGBoost Test Metrics ===
MAE: 0.4502
RMSE: 0.7236
R2: 0.9969

=== XGBoost Cross-validation ===
Cross-validated R² scores: [0.91636526 0.92531532 0.90206774 0.94479694 0.89777472]
Mean R²: 0.9173 | Std: 0.0169

=== Tuning and evaluating LightGBM ===
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000161 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 111
[LightGBM] [Info] Number of data points in the train set: 1519, number of used features: 5
[LightGBM] [Info] Start training from score 23.570178
Best parameters for LightGBM: {'learning_rate': 0.2, 'n_estimators': 200, 'num_leaves': 31}

=== LightGBM Train Metrics ===
MAE: 0.2372
RMSE: 0.3451
R2: 0.9993

=== LightGBM Validation Metrics ===
MAE: 0.5289
RMSE: 0.8181
R2: 0.9958

=== LightGBM Test Metrics ===
MAE: 0.4540
RMSE: 0.6975
R2: 0.9971

=== LightGBM Cross-validation ===
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000135 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 97
[LightGBM] [Info] Number of data points in the train set: 1736, number of used features: 5
[LightGBM] [Info] Start training from score 23.478111
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 100
[LightGBM] [Info] Number of data points in the train set: 1736, number of used features: 5
[LightGBM] [Info] Start training from score 23.751498
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 106
[LightGBM] [Info] Number of data points in the train set: 1736, number of used features: 5
[LightGBM] [Info] Start training from score 23.487730
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 106
[LightGBM] [Info] Number of data points in the train set: 1736, number of used features: 5
[LightGBM] [Info] Start training from score 24.610023
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 100
[LightGBM] [Info] Number of data points in the train set: 1736, number of used features: 5
[LightGBM] [Info] Start training from score 22.920795
Cross-validated R² scores: [0.93013247 0.93231064 0.95454214 0.90441284 0.76838626]
Mean R²: 0.8980 | Std: 0.0667

=== Tuning and evaluating CatBoost ===
/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names
  warnings.warn(
Best parameters for CatBoost: {'depth': 5, 'iterations': 200, 'learning_rate': 0.2}

=== CatBoost Train Metrics ===
MAE: 0.4034
RMSE: 0.5538
R2: 0.9982

=== CatBoost Validation Metrics ===
MAE: 0.6158
RMSE: 0.8995
R2: 0.9949

=== CatBoost Test Metrics ===
MAE: 0.5679
RMSE: 0.8220
R2: 0.9960

=== CatBoost Cross-validation ===
Cross-validated R² scores: [0.90736911 0.92905921 0.9325161  0.93924826 0.87067857]
Mean R²: 0.9158 | Std: 0.0249

=== All Models Comparison ===
                           Train_MAE  Train_RMSE  Train_R2   Val_MAE  \
GradientBoostingRegressor   0.150933    0.205325  0.999756  0.496369   
XGBoost                     0.171045    0.237148  0.999674  0.528958   
LightGBM                    0.237166    0.345121  0.999311  0.528920   
CatBoost                    0.403389    0.553847  0.998225  0.615795   

                           Val_RMSE    Val_R2  Test_MAE  Test_RMSE   Test_R2  
GradientBoostingRegressor  0.809597  0.995906  0.430323   0.638652  0.997603  
XGBoost                    0.844202  0.995548  0.450248   0.723553  0.996924  
LightGBM                   0.818051  0.995820  0.454010   0.697515  0.997141  
CatBoost                   0.899538  0.994946  0.567877   0.822012  0.996030  