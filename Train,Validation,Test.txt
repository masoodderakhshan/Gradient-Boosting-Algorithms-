import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt # Keep for initial style setup, though no plots will be shown
from itertools import product # To generate combinations

# --- Add new imports for Boosting Machines ---
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
import lightgbm as lgb
import catboost as cb

# === Set plot style for article (kept for consistency with original aesthetic setup) ===
plt.rcParams['font.family'] = 'DejaVu Serif'
plt.rcParams['font.serif'] = ['Times New Roman']
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 10
plt.rcParams['figure.titlesize'] = 14
plt.rcParams['axes.titlepad'] = 15

# === File paths ===
weka3_filepath = r"/content/WEKA-3.csv"
weka4_filepath = r"/content/WEKA-4.csv"

# === Load features ===
print("\n=== Loading input features ===")
df_features = pd.read_csv(weka3_filepath)
print(df_features.head())

# === Load displacement ===
print("\n=== Loading displacement data ===")
df_raw = pd.read_csv(weka4_filepath, header=None)
test_ids = df_raw.iloc[0, 1::2].astype(int).tolist()
disp_steps = df_raw.iloc[2:, 0].astype(float)
kpa_cols = df_raw.iloc[2:, 1::2].astype(float)

print("\n=== Reshaping displacement data ===")
dfs = []
for idx, test_id in enumerate(test_ids):
    df_temp = pd.DataFrame({
        'Displacement_Step': disp_steps,
        'Displacement_kPa': kpa_cols.iloc[:, idx],
        'Test': test_id
    })
    dfs.append(df_temp)
df_displacement = pd.concat(dfs, ignore_index=True)
print(df_displacement.head())

# === Merge ===
df = pd.merge(df_displacement, df_features, on='Test', how='left')
print(df.head())

# === Missing handling ===
print("\n=== Checking missing ===")
print(df.isnull().sum())
for col in df.select_dtypes(include=[np.number]).columns:
    if df[col].isnull().any():
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)
        print(f"Filled missing in {col} with median {median_val}")

# === Features and target ===
features = ['NS', 'Depth', 'W', 'RWD', 'Displacement_Step']
X = df[features]
y = df['Displacement_kPa']

# === Scale ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) # Fit scaler on the full dataset X

# === Train/Val/Test split ===
# This split is still necessary for training the models
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"\nTrain: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")


# === Initialize Models ===
models = {
    'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42),
    'XGBoost': XGBRegressor(random_state=42),
    'LightGBM': lgb.LGBMRegressor(random_state=42),
    'CatBoost': cb.CatBoostRegressor(random_seed=42, verbose=0)
}

trained_models = {} # To store trained models
results = {} # To store evaluation metrics

for name, model in models.items():
    print(f"\n=== Tuning and evaluating {name} ===")

    # Define param grid based on model
    if name in ['GradientBoostingRegressor', 'XGBoost']:
        param_grid = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 4, 5]
        }
    elif name == 'LightGBM':
        param_grid = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'num_leaves': [15, 31, 63]
        }
    elif name == 'CatBoost':
        param_grid = {
            'iterations': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'depth': [3, 4, 5]
        }

    # Perform grid search
    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    print(f"Best parameters for {name}: {grid_search.best_params_}")

    best_model = grid_search.best_estimator_
    trained_models[name] = best_model # Store the trained model

    # Predict and evaluate on train set
    y_train_pred = best_model.predict(X_train)
    mae_train = mean_absolute_error(y_train, y_train_pred)
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    r2_train = r2_score(y_train, y_train_pred)

    print(f"\n=== {name} Train Metrics ===")
    print(f"MAE: {mae_train:.4f}")
    print(f"RMSE: {rmse_train:.4f}")
    print(f"R2: {r2_train:.4f}")

    # Predict and evaluate on validation set
    y_val_pred = best_model.predict(X_val)
    mae_val = mean_absolute_error(y_val, y_val_pred)
    rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))
    r2_val = r2_score(y_val, y_val_pred)

    print(f"\n=== {name} Validation Metrics ===")
    print(f"MAE: {mae_val:.4f}")
    print(f"RMSE: {rmse_val:.4f}")
    print(f"R2: {r2_val:.4f}")

    # Predict and evaluate on test set
    y_test_pred = best_model.predict(X_test)
    mae_test = mean_absolute_error(y_test, y_test_pred)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
    r2_test = r2_score(y_test, y_test_pred)

    print(f"\n=== {name} Test Metrics ===")
    print(f"MAE: {mae_test:.4f}")
    print(f"RMSE: {rmse_test:.4f}")
    print(f"R2: {r2_test:.4f}")

    results[name] = {
        'Train_MAE': mae_train, 'Train_RMSE': rmse_train, 'Train_R2': r2_train,
        'Val_MAE': mae_val, 'Val_RMSE': rmse_val, 'Val_R2': r2_val,
        'Test_MAE': mae_test, 'Test_RMSE': rmse_test, 'Test_R2': r2_test
    }

    # === Cross-validation ===
    print(f"\n=== {name} Cross-validation ===")
    cv_scores = cross_val_score(best_model, X_scaled, y, cv=5, scoring='r2')
    print(f"Cross-validated R² scores: {cv_scores}")
    print(f"Mean R²: {cv_scores.mean():.4f} | Std: {cv_scores.std():.4f}")

print("\n=== All Models Comparison ===")
results_df = pd.DataFrame(results).T
print(results_df)


# ==============================================================================
# ==============================================================================
# === Predict and Plot LightGBM 'Displacement_kPa' vs 'Displacement_Step' Curves ===
# ==============================================================================
# ==============================================================================
# === Predict and Plot LightGBM 'Displacement_kPa' vs 'Displacement_Step' Curves ===
# ==============================================================================

print("\n=== Predicting and plotting LightGBM Displacement-Slip curves ===")

# --- Define scenario parameters ---
scenario_params = {
    'NS': [20, 80],
    'Depth': [5, 15],
    'W': [30, 40],
    'RWD': [0, 1, 3]
}

# --- Generate displacement steps for smooth curve ---
disp_steps = np.linspace(df['Displacement_Step'].min(), df['Displacement_Step'].max(), 50)

# --- Prepare combinations ---
combinations = list(product(
    scenario_params['NS'],
    scenario_params['Depth'],
    scenario_params['W'],
    scenario_params['RWD']
))

# --- Select trained LightGBM model ---
lgb_model = trained_models['LightGBM']

# --- Create figure layout (2x2 grid for Depth × W) ---
fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)
axes = axes.flatten()

# --- Plot curves for each (Depth, W) combination ---
for i, (depth, W) in enumerate(product(scenario_params['Depth'], scenario_params['W'])):
    ax = axes[i]
    for NS in scenario_params['NS']:
        for RWD in scenario_params['RWD']:
            # Generate data for one curve
            data = pd.DataFrame({
                'NS': [NS] * len(disp_steps),
                'Depth': [depth] * len(disp_steps),
                'W': [W] * len(disp_steps),
                'RWD': [RWD] * len(disp_steps),
                'Displacement_Step': disp_steps
            })

            # Scale using the same scaler
            data_scaled = scaler.transform(data)

            # Predict displacement
            y_pred = lgb_model.predict(data_scaled)

            # Plot curve
            ax.plot(disp_steps, y_pred,
                    label=f'NS={NS} | RWD={RWD}',
                    linewidth=1.8)

    ax.set_title(f'Depth={depth} cm, W={W}%', fontsize=12)
    ax.set_xlabel('Displacement Step', fontsize=11)
    ax.set_ylabel('Predicted Shear Stress (kPa)', fontsize=11)
    ax.legend(frameon=False, fontsize=8)
    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)

# --- Final layout ---
fig.suptitle("Predicted 'Displacement_kPa'–'Displacement_Step' Curves (LightGBM)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.97])

# --- Save figure at 500 DPI ---
output_fig_path = "LightGBM_Displacement_Slip_Curves_500dpi.png"
plt.savefig(output_fig_path, dpi=500, bbox_inches='tight')
print(f"\nFigure saved successfully as '{output_fig_path}' (500 DPI).")

plt.show()